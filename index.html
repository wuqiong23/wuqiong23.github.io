<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="wuqiong&#39;Zone">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="wuqiong&#39;Zone">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="wuqiong&#39;Zone">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>wuqiong'Zone</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">wuqiong'Zone</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/06/test2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wuqiong'Zone">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/03/06/test2/" itemprop="url">test2</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-06T11:46:52+08:00">
                2018-03-06
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><span class="math inline">\(F_{\mu}\)</span></p>
<p><span class="math inline">\(F\_a + F\_b = F\_c\)</span></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/06/LDA_test/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wuqiong'Zone">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/03/06/LDA_test/" itemprop="url">未命名</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-06T11:33:42+08:00">
                2018-03-06
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="线性判别分析lda总结">线性判别分析LDA总结</h1>
<p>  Fisher‘s Linear Discriminant (FLD) 是统计学上的经典分析方法，LDA在FLD的基础上增加了关于变量分布和协方差的假设：1.样本数据服从正态分布，2.各类的协方差相等。虽然这些假设在实际中不一定满足，但是LDA既可以用来作线性分类，也可以用来对数据进行降维。下面，我们从LDA的思想，数学基础，原理，算法流程，python实现，使用LDA的限制以及与PCA的对比等方面对LDA进行总结。 ## LDA的思想   LDA是一种监督学习的降维技术，也就是说它的数据集的每个样本是有类别输出的。LDA的思想可以用一句话概括，就是“投影后类内方差最小，类间方差最大”。也就是说，我们要将数据在低维度上进行投影，投影后希望每一种类别数据的投影点尽可能的接近，而不同类别的数据的类别中心之间的距离尽可能的大。举例来说，假设我们有两类数据 分别为红色和蓝色，如下图所示，这些数据特征是二维的，我们希望将这些数据投影到一维的一条直线，让每一种类别数据的投影点尽可能的接近，而红色和蓝色数据中心之间的距离尽可能的大。<br> <img src="attachment:/myimages/LDA/LDA.png" alt="LDA"></p>
<p>  上图提供了两种投影方式，从直观上可以看出，右图要比左图的投影效果好，因为右图的同色数据较为集中，且类别之间的距离明显。左图则在边界处数据混杂。<br>   在实际应用中，我们的数据是多个类别的，我们的原始数据一般也是超过二维的，投影后的也一般不是直线，而是一个低维的超平面。</p>
<h2 id="数学基础">数学基础</h2>
<p>  在具体讲解LDA原理之前，我们先补充一些相关的数学基础知识——瑞利商（Rayleigh quotient）与广义瑞利商（genralized Rayleigh quotient）<br> ### 瑞利商   瑞利商的函数如下： <span class="math display">\[R(A,x)= \frac{x^{T}Ax}{x^{T}x}\]</span>   其中x为非零向量，而A为<span class="math inline">\(n*n\)</span>的Hermitan矩阵。所谓的Hermitan矩阵就是满足共轭转置矩阵和自己相等的矩阵，即<span class="math inline">\(A^{T}=A\)</span>。如果我们的矩阵A是实矩阵，则满足<span class="math inline">\(A^{T}=A\)</span>的矩阵即为Hermitan矩阵。<br>   瑞利商R(A,x)有一个非常重要的性质，即它的最大值等于矩阵A最大的特征值，而最小值等于矩阵A的最小的特征值，也就是满足:</p>
<p><span class="math display">\[ \lambda_{min}\le\frac{x^{T}Ax}{x^{T}x}\le \lambda_{max}\]</span></p>
<p>  当向量x是标准正交基时，即满足<span class="math inline">\(x^{T}x = 1\)</span>时，瑞利商简化为：<span class="math inline">\(R(A,x)= x^{T}Ax\)</span>。</p>
<h3 id="广义瑞利商">广义瑞利商</h3>
<p>  广义瑞利商的函数如下： <span class="math display">\[R(A,B,x)= \frac{x^{T}Ax}{x^{T}Bx}\]</span>   其中x为非零向量，而A，B为<span class="math inline">\(n*n\)</span>的Hermitan矩阵，B为正定矩阵。我们可以将其通过标准化转化为瑞利商的格式。我们令<span class="math inline">\(x^{&#39;}=B^{-1/2}x\)</span>，则分母转化为：<br></p>
<p><span class="math display">\[x^{T}Bx = x^{&#39;T}(B^{-1/2})^{T}B(B^{-1/2})x^{&#39;}= x^{&#39;T}B^{-1/2}B(B^{-1/2})x^{&#39;}= x^{&#39;T}x^{&#39;}\]</span></p>
<p>  分子转化为： <span class="math display">\[x^{T}Ax = x^{&#39;T}B^{-1/2}AB^{-1/2}x^{&#39;}\]</span></p>
<p>  此时<span class="math inline">\(R(A,B,x)\)</span>转化为<span class="math inline">\(R(A,B,x^{&#39;})\)</span>：</p>
<p><span class="math display">\[R(A,B,x^{&#39;})= \frac{x^{&#39;T}B^{-1/2}AB^{-1/2}x^{&#39;}}{x^{&#39;T}x^{&#39;}}\]</span></p>
<p>  利用前面的瑞利商的性质，可知，<span class="math inline">\(R(A,B,x)\)</span>的最大值为矩阵<span class="math inline">\(B^{-1/2}AB^{-1/2}\)</span>，也即矩阵<span class="math inline">\(B^{-1}A\)</span>的最大特征值。而最小值为矩阵<span class="math inline">\(B^{-1}A\)</span>的最小特征值。</p>
<h2 id="lda原理">LDA原理</h2>
<p>### 二分类原理   假设我们的数据集<span class="math inline">\(D = \{(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{m},y_{m})\}\)</span>。其中任意样本<span class="math inline">\(x_{i}\)</span>为n维向量，<span class="math inline">\(y_{i}\in\{0，1\}\)</span>。我们定义<span class="math inline">\(N_{j}(j=0,1)\)</span>为第j类样本的个数，<span class="math inline">\(X_{j}(j=0,1)\)</span>为第j类样本的集合，而<span class="math inline">\(\mu_{j}(j=0,1)\)</span>为第j类样本的均值向量，定义<span class="math inline">\(\sum_{j}(j=0,1)\)</span>为第j类样本的协方差矩阵（严格说是缺少分母部分的协方差矩阵）。<br> <span class="math inline">\(\mu_{j}\)</span>的表达式为：</p>
<p><span class="math display">\[\mu_{j} = \frac{1}{N_{j}}\sum_{x \in X_{j}}x(j=0,1)\]</span></p>
<p>  <span class="math inline">\(\sum_{j}\)</span>的表达式为：</p>
<p><span class="math display">\[\sum_{j} = \sum_{x \in X_{j}}(x-\mu_{j})(x-\mu_{j})^{T}(j=0,1)\]</span>   由于是两类数据，因此我们只需要将数据投影到一条直线上即可。假设我们的投影直线是向量w，则对任意一个样本<span class="math inline">\(x_{i}\)</span>，它在直线w的投影为<span class="math inline">\(w^{T}x_{i}\)</span>，对于我们的两个类别的中心点<span class="math inline">\(\mu_{0}\)</span>，<span class="math inline">\(\mu_{1}\)</span>，在直线w的投影为<span class="math inline">\(w^{T}\mu_{0}\)</span>和<span class="math inline">\(w^{T}\mu_{1}\)</span>。由于LDA需要让不同类别的数据的类别中心之间的距离尽可能的大，也就是我们要最大化<span class="math inline">\(||w^{T}\mu_{0}-w^{T}\mu_{1}||^{2}_{2}\)</span>，同时我们希望同一种类别数据的投影点尽可能的接近，也就是要同类样本投影点的协方差<span class="math inline">\(w^{T}\sum_{0}w\)</span>和<span class="math inline">\(w^{T}\sum_{1}w\)</span>尽可能的小，即最小化<span class="math inline">\(w^{T}\sum_{0}w + w^{T}\sum_{1}w\)</span>。<br></p>
<p>  综上所述，我们的优化目标为： <span class="math display">\[\mathop{\arg\max}_wJ(W)=\frac{||w^{T}\mu_{0}-w^{T}\mu_{1}||^{2}_{2}}{w^{T}\sum_{0}w + w^{T}\sum_{1}w} = \frac{w^{T}(\mu_{0}-\mu_{1})(\mu_{0}-\mu_{1})^{T}w}{w^{T}(\sum_{0}+\sum_{1})w}\]</span><br></p>
<p>  我们一般定义类内散度矩阵<span class="math inline">\(S_{w}\)</span>为：</p>
<p><span class="math display">\[S_{w} = \sum_{0}+\sum_{1} = \sum_{x \in X_{0}}(x - \mu_{0})(x - \mu_{0})^{T} + \sum_{x \in X_{1}}(x - \mu_{1})(x - \mu_{1})^{T}\]</span></p>
<p>  同时定义类间散度矩阵<span class="math inline">\(S_{b}\)</span>为： <span class="math display">\[S_{b}= (\mu_{0}-\mu_{1})(\mu_{0}-\mu_{1})^{T}\]</span></p>
<p>  这样，我们的优化目标重写为：</p>
<p><span class="math display">\[\mathop{\arg\max}_wJ(W)= \frac{w^{T}S_{b}w}{w^{T}S_{w}w}\]</span><br></p>
<p>  可以看出，上式即为广义瑞利商的形式，我们利用广义瑞利商的性质可知，<span class="math inline">\(J(w)\)</span>的最大值为矩阵<span class="math inline">\(S_{w}^{-1}S_{b}\)</span>的最大特征值，而对应的w为<span class="math inline">\(S_{w}^{-1}S_{b}\)</span>的最大特征值对应的特征向量。</p>
<p>  对于二类的时候，<span class="math inline">\(S_{b}w\)</span>的方向恒为<span class="math inline">\(\mu_{0}-\mu_{1}\)</span>，不妨令<span class="math inline">\(S_{b}w = \lambda(\mu_{0}-\mu_{1})\)</span>，将其带入：<span class="math inline">\((S_{w}^{-1}S_{b})w = \lambda w\)</span>，可以得到<span class="math inline">\(w = S_{w}^{-1}(\mu_{0}-\mu_{1})\)</span>，也就是说我们只要求出原始二类样本的均值和方差就可以确定最佳的投影方向w了。</p>
<p>### 多分类原理   有了二类LDA的基础，我们再来看看多类别LDA的原理。<br><br>   假设我们的数据集 $ D = {(x_{1},y_{1}),,(x_{2},y_{2}),,...,,(x_{m},y_{m})}$ 其中任意样本<span class="math inline">\(x_{i}\)</span>为n维向量，<span class="math inline">\(y_{i}\in\{C_{1},\,C_{2},\,...,\,C_{k}\}\)</span>。我们定义<span class="math inline">\(N_{j}(j=1,2,...,k)\)</span>为第j类样本的个数，<span class="math inline">\(X_{j}(j=1,2,...,k)\)</span>为第j类样本的集合，而<span class="math inline">\(\mu_{j}(j=1,2,...,k)\)</span>为第j类样本的均值向量，定义为<span class="math inline">\(\sigma_{j}(j=1,2,...,k)\)</span>第j类样本的协方差矩阵。在二类LDA里面定义的公式可以很容易的类推到多类LDA。<br><br>   由于我们是多类向低维投影，则此时投影到的低维空间就不是一条直线，而是一个超平面。假设我们投影到的低维空间的维度为d，对应的基向量为<span class="math inline">\((w_{1},\,w_{2},\,...,\,w_{d})\)</span>，基向量的矩阵为<span class="math inline">\(W\)</span>，他是一个<span class="math inline">\(n\times d\)</span>的矩阵。<br>   此时我们的优化目标应该可以变成为： <span class="math display">\[\frac{W^T S_{b}W}{W^T S_{w}W}\]</span><br>   其中<span class="math inline">\(S_{b}=\sum_{j=1}^k N_{j}(\mu_j-\mu)(\mu_j-\mu)^T\)</span>,<span class="math inline">\(\mu\)</span>为所有样本的均值向量。而<span class="math inline">\(S_w=\sum_{j=1}^kS_{wj}=\sum_{j=1}^k\sum_{x\in X_j}(x-\mu_j)(x-\mu_j)^T\)</span>。<br><br>   但是有一个问题，就是<span class="math inline">\(W^T S_{b}W\)</span>和<span class="math inline">\(W^T S_{w}W\)</span>都是矩阵，不是标量，无法作为一个标量函数来优化！也就是说，我们无法直接用二类LDA的优化方法，怎么办呢？一般来说，我们可以用其他的一些替代优化目标来实现。<br><br>   常见的一个LDA多类优化目标函数定义为: <span class="math display">\[\mathop{\arg\max}_WJ(W)=\frac{\prod_{diag}W^TS_bW}{\prod_{diag}W^TS_wW}\]</span><br><br>   其中<span class="math inline">\(\prod_{diag}A\)</span>为<span class="math inline">\(A\)</span>的主对角线元素的乘积，<span class="math inline">\(W\)</span>为<span class="math inline">\(n\times d\)</span>的矩阵。<br><br>   <span class="math inline">\(J(W)\)</span>的优化过程可以转化为： <span class="math display">\[J(W)=\frac{\prod_{i=1}^dw_i^TS_bw_i}{\prod_{i=1}^dw_i^TS_ww_i}=\prod_{i=1}^d\frac{w_i^TS_bw_i}{w_i^TS_ww_i}\]</span> <br><br>   仔细观察上式最右边，这正是广义瑞利商。最大值是矩阵<span class="math inline">\(S_w^{-1}S_b\)</span>的最大特征值，最大的d个值的乘积就是矩阵<span class="math inline">\(S_w^{-1}S_b\)</span>的最大的d个特征值的乘积，此时对应的矩阵<span class="math inline">\(W\)</span>为这最大的d个特征值对应的特征向量张成的矩阵。<br><br>   由于<span class="math inline">\(W\)</span>是一个利用了样本的类别得到的投影矩阵，因此它降维到的维度d最大值为k-1。为什么最大维度不是类别数k呢？因为<span class="math inline">\(S_b\)</span>中每个<span class="math inline">\(\mu_j-\mu\)</span>的秩为1，因此协方差矩阵相加后最大的秩为k（矩阵的秩小于等于各个相加矩阵的秩的和），但是由于如果我们知道前k-1个<span class="math inline">\(\mu_j\)</span>后，最后一个<span class="math inline">\(\mu_k\)</span>可以由前k-1个<span class="math inline">\(\mu_j\)</span>线性表示，因此<span class="math inline">\(S_b\)</span>的秩最大为k-1，即特征向量最多由k-1个。</p>
<h2 id="lda算法流程">LDA算法流程</h2>
<p>  在上面我们已经讲述了LDA的原理，现在我们对LDA降维的流程做一个总结。<br>   输入：数据集 $ D = {(x_{1},y_{1}),,(x_{2},y_{2}),,...,,(x_{m},y_{m})}<span class="math inline">\(，其中其中任意样本\)</span>x_{i}<span class="math inline">\(为n维向量，\)</span>y_{i}{C_{1},,C_{2},,...,,C_{k}}<span class="math inline">\(，降维到维度d。&lt;br&gt; &amp;emsp;&amp;emsp;输出：降维后的样本集\)</span>D'$ <br>   1) 计算类内散度矩阵<span class="math inline">\(S_w\)</span><br>   2) 计算类间散度矩阵<span class="math inline">\(S_b\)</span><br>   3) 计算矩阵<span class="math inline">\(S_w^{-1}S_b\)</span><br>   4) 计算<span class="math inline">\(S_w^{-1}S_b\)</span>的最大的d个特征值和对应的d个特征向量<span class="math inline">\((w_{1},\,w_{2},\,...,\,w_{d})\)</span>，得到投影矩阵<span class="math inline">\(W\)</span><br>   5) 对样本集中的每一个样本特征<span class="math inline">\(x_i\)</span>，转化为新的样本<span class="math inline">\(z_i = W^Tx_i\)</span><br>   6) 得到输出样本集<span class="math inline">\(D = \{(z_{1},y_{1}),\,(z_{2},y_{2}),\,...,\,(z_{m},z_{m})\}\)</span><br><br>   以上就是使用LDA进行降维的算法流程。实际上LDA除了可以用于降维之外，还可以用于分类。一个常见的LDA分类基本思想是假设各个类别的样本数据符合高斯分布，这样利用LDA进行投影后，可以利用极大似然估计计算各个类别投影数据的均值和方差，进而得到该类别高斯分布的概率密度函数。当一个新的样本到来后，我们可以将它投影，然后将投影后的样本特征分别代入各个类别的高斯分布概率密度函数中，计算出它属于这个类别的概率，最大的概率对应的类别即为预测类别。</p>
<h2 id="python实现">python实现</h2>
<p>  在scikit-learn中， LDA类是sklearn.discriminant_analysis.LinearDiscriminantAnalysis。那既可以用于分类又可以用于降维。当然，应用场景最多的还是降维。和PCA类似，LDA降维基本也不用调参，只需要指定降维到的维数即可。<br>   LinearDiscriminantAnalysis类的参数如下：<br>   1）solver : 即求LDA超平面特征矩阵使用的方法。可以选择的方法有奇异值分解&quot;svd&quot;，最小二乘&quot;lsqr&quot;和特征分解&quot;eigen&quot;。一般来说特征数非常多的时候推荐使用svd，而特征数不多的时候推荐使用eigen。主要注意的是，如果使用svd，则不能指定正则化参数shrinkage进行正则化。默认值是svd。</p>
<p>  2）shrinkage：正则化参数，可以增强LDA分类的泛化能力。如果仅仅只是为了降维，则一般可以忽略这个参数。默认是None，即不进行正则化。可以选择&quot;auto&quot;,让算法自己决定是否正则化。当然我们也可以选择不同的[0,1]之间的值进行交叉验证调参。注意shrinkage只在solver为最小二乘&quot;lsqr&quot;和特征分解&quot;eigen&quot;时有效。</p>
<p>  3）priors ：类别权重，可以在做分类模型时指定不同类别的权重，进而影响分类模型建立。降维时一般不需要关注这个参数。</p>
<p>  4）n_components：即我们进行LDA降维时降到的维数。在降维时需要输入这个参数。注意只能为[1,类别数-1)范围之间的整数。如果我们不是用于降维，则这个值可以用默认的None。</p>
<p>  从上面的描述可以看出，如果我们只是为了降维，则只需要输入n_components,注意这个值必须小于“类别数-1”。PCA没有这个限制。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#生成三类三维特征的数据</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets.samples_generator <span class="keyword">import</span> make_classification</span><br><span class="line">X, y = make_classification(n_samples=<span class="number">1000</span>, n_features=<span class="number">3</span>, n_redundant=<span class="number">0</span>, n_classes=<span class="number">3</span>, n_informative=<span class="number">2</span>,</span><br><span class="line">                           n_clusters_per_class=<span class="number">1</span>,class_sep =<span class="number">0.5</span>, random_state =<span class="number">10</span>)</span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = Axes3D(fig, rect=[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>], elev=<span class="number">30</span>, azim=<span class="number">20</span>)</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], X[:, <span class="number">2</span>],marker=<span class="string">'o'</span>,c=y)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x110b1f490&gt;</code></pre>
<div class="figure">
<img src="output_5_1.png" alt="png">
<p class="caption">png</p>
</div>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用PCA降维到二维，注意PCA无法使用类别信息来降维</span></span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>)</span><br><span class="line">pca.fit(X)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"两个主成分方差比："</span></span><br><span class="line"><span class="keyword">print</span> pca.explained_variance_ratio_</span><br><span class="line"><span class="keyword">print</span> <span class="string">"两个主成分方差："</span></span><br><span class="line"><span class="keyword">print</span> pca.explained_variance_</span><br><span class="line">X_new = pca.transform(X)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"PCA降维效果图："</span></span><br><span class="line">plt.scatter(X_new[:, <span class="number">0</span>], X_new[:, <span class="number">1</span>],marker=<span class="string">'o'</span>,c=y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>两个主成分方差比：
[ 0.43377069  0.3716351 ]
两个主成分方差：
[ 1.21083449  1.0373882 ]
PCA降维效果图：</code></pre>
<div class="figure">
<img src="output_6_1.png" alt="png">
<p class="caption">png</p>
</div>
<p>由于PCA没有利用类别信息，我们可以看到降维后，样本特征和类别的信息关联几乎完全丢失。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用LDA降维到二维</span></span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> LinearDiscriminantAnalysis</span><br><span class="line">lda = LinearDiscriminantAnalysis(n_components=<span class="number">2</span>)</span><br><span class="line">lda.fit(X,y)</span><br><span class="line">X_new = lda.transform(X)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"LDA降维效果图："</span></span><br><span class="line">plt.scatter(X_new[:, <span class="number">0</span>], X_new[:, <span class="number">1</span>],marker=<span class="string">'o'</span>,c=y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>LDA降维效果图：</code></pre>
<div class="figure">
<img src="output_8_1.png" alt="png">
<p class="caption">png</p>
</div>
<p>可以看出降维后样本特征和类别信息之间的关系得以保留。</p>
<h2 id="使用lda的限制">使用LDA的限制</h2>
<ul>
<li>LDA至多生成C-1维子空间</li>
<li>LDA不适合对非高斯分布的样本进行降维</li>
<li>LDA在样本分类信息依赖方差而不是均值时，效果不好</li>
<li>LDA可能过度拟合数据</li>
</ul>
<h2 id="lda与pca的对比">LDA与PCA的对比</h2>
<table style="width:58%;">
<colgroup>
<col width="8%">
<col width="8%">
<col width="8%">
<col width="8%">
<col width="8%">
<col width="8%">
<col width="8%">
</colgroup>
<thead>
<tr class="header">
<th align="center">降维方法</th>
<th align="center">思想</th>
<th align="center">分布</th>
<th align="center">监督方式</th>
<th align="center">投影</th>
<th align="center">维度</th>
<th align="center">目的</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">PCA</td>
<td align="center">数据降维，矩阵特征分解</td>
<td align="center">假设数据符合高斯分布</td>
<td align="center">无监督</td>
<td align="center">投影的坐标系都是正交的</td>
<td align="center">直接和特征维度相关，比如原始数据是d维，PCA之后，可以任意选取1～n维</td>
<td align="center">去除原始数据集中冗余的维度，让投影子空间各个维度的方差尽可能大</td>
</tr>
<tr class="even">
<td align="center">LDA</td>
<td align="center">数据降维，矩阵特征分解</td>
<td align="center">假设数据符合高斯分布</td>
<td align="center">有监督</td>
<td align="center">根据类别的标注关注分类能力，不保证投影到的坐标系是正交的</td>
<td align="center">与数据本身的维度无关，和类别个数C相关，LDA之后，在1～C-1维进行选择</td>
<td align="center">找到具有区分的维度，使得原始数据在这些维度上的投影能尽可能区分不同类别</td>
</tr>
</tbody>
</table>
<p>  如下图所示，是PCA和LDA分别选择的投影方向。 <img src="attachment:PCAandLDA.jpg" alt="PCAandLDA.jpg"></p>
<h2 id="参考资料">参考资料</h2>
<ol style="list-style-type: decimal">
<li><a href="https://www.cnblogs.com/pinard/p/6244265.html" target="_blank" rel="noopener">线性判别分析LDA原理总结</a></li>
<li><a href="https://mp.weixin.qq.com/s/CkDzLZCXOF6zzrn6_dd6Jw?utm_source=tuicool&amp;utm_medium=referral" target="_blank" rel="noopener">一文读懂特征工程</a></li>
<li><a href="https://www.cnblogs.com/engineerLF/p/5393119.html" target="_blank" rel="noopener">线性判别分析LDA详解</a></li>
<li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html" target="_blank" rel="noopener">LinearDiscriminantAnalysis</a></li>
<li><a href="http://blog.csdn.net/lhz76ttw1u/article/details/60768981" target="_blank" rel="noopener">用scikit-learn进行LDA降维</a></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/05/SNE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wuqiong'Zone">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/03/05/SNE/" itemprop="url">SNE</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-05T20:28:09+08:00">
                2018-03-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/05/LDA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wuqiong'Zone">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/03/05/LDA/" itemprop="url">LDA总结</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-05T20:08:57+08:00">
                2018-03-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="线性判别分析lda总结">线性判别分析LDA总结</h1>
<p>  Fisher‘s Linear Discriminant (FLD) 是统计学上的经典分析方法<span class="math inline">\(T(n) = \Theta(n)\)</span>，<span class="math inline">\(x_i\)</span>，LDA在FLD的基础上增加了关于变量分布和协方差的假设：1.样本数据服从正态分布，2.各类的协方差相等。虽然这些假设在实际中不一定满足，但是LDA既可以用来作线性分类，也可以用来对数据进行降维。下面，我们从LDA的思想，数学基础，原理，算法流程，python实现，使用LDA的限制以及与PCA的对比等方面对LDA进行总结。 ## LDA的思想   LDA是一种监督学习的降维技术，也就是说它的数据集的每个样本是有类别输出的。LDA的思想可以用一句话概括，就是“投影后类内方差最小，类间方差最大”。也就是说，我们要将数据在低维度上进行投影，投影后希望每一种类别数据的投影点尽可能的接近，而不同类别的数据的类别中心之间的距离尽可能的大。举例来说，假设我们有两类数据 分别为红色和蓝色，如下图所示，这些数据特征是二维的，我们希望将这些数据投影到一维的一条直线，让每一种类别数据的投影点尽可能的接近，而红色和蓝色数据中心之间的距离尽可能的大。<br> <img src="/myimages/LDA.png" alt="LDA"></p>
<p>  上图提供了两种投影方式，从直观上可以看出，右图要比左图的投影效果好，因为右图的同色数据较为集中，且类别之间的距离明显。左图则在边界处数据混杂。<br>   在实际应用中，我们的数据是多个类别的，我们的原始数据一般也是超过二维的，投影后的也一般不是直线，而是一个低维的超平面。 ## 数学基础   在具体讲解LDA原理之前，我们先补充一些相关的数学基础知识——瑞利商（Rayleigh quotient）与广义瑞利商（genralized Rayleigh quotient）<br> ### 瑞利商   瑞利商的函数如下： <span class="math display">\[R(A,x)= \frac{x^{T}Ax}{x^{T}x}\]</span>   其中x为非零向量，而A为<span class="math inline">\(n*n\)</span>的Hermitan矩阵。所谓的Hermitan矩阵就是满足共轭转置矩阵和自己相等的矩阵，即<span class="math inline">\(A^{T}=A\)</span>。如果我们的矩阵A是实矩阵，则满足<span class="math inline">\(A^{T}=A\)</span>的矩阵即为Hermitan矩阵。<br>   瑞利商R(A,x)有一个非常重要的性质，即它的最大值等于矩阵A最大的特征值，而最小值等于矩阵A的最小的特征值，也就是满足:</p>
<p><span class="math display">\[\lambda_{min}\le\frac{x^{T}Ax}{x^{T}x}\le \lambda_{max}\]</span></p>
<p>  当向量x是标准正交基时，即满足<span class="math inline">\(x^{T}x = 1\)</span>时，瑞利商简化为：<span class="math inline">\(R(A,x)= x^{T}Ax\)</span>。</p>
<h3 id="广义瑞利商">广义瑞利商</h3>
<p>  广义瑞利商的函数如下： <span class="math display">\[R(A,B,x)= \frac{x^{T}Ax}{x^{T}Bx}\]</span>   其中x为非零向量，而A，B为<span class="math inline">\(n*n\)</span>的Hermitan矩阵，B为正定矩阵。我们可以将其通过标准化转化为瑞利商的格式。我们令<span class="math inline">\(x^{&#39;}=B^{-1/2}x\)</span>，则分母转化为：<br></p>
<p><span class="math display">\[x^{T}Bx = x^{&#39;T}(B^{-1/2})^{T}B(B^{-1/2})x^{&#39;}= x^{&#39;T}B^{-1/2}B(B^{-1/2})x^{&#39;}= x^{&#39;T}x^{&#39;}\]</span></p>
<p>  分子转化为： <span class="math display">\[x^{T}Ax = x^{&#39;T}B^{-1/2}AB^{-1/2}x^{&#39;}\]</span></p>
<p>  此时<span class="math inline">\(R(A,B,x)\)</span>转化为<span class="math inline">\(R(A,B,x^{&#39;})\)</span>：</p>
<p><span class="math display">\[R(A,B,x^{&#39;})= \frac{x^{&#39;T}B^{-1/2}AB^{-1/2}x^{&#39;}}{x^{&#39;T}x^{&#39;}}\]</span></p>
<p>  利用前面的瑞利商的性质，可知，<span class="math inline">\(R(A,B,x)\)</span>的最大值为矩阵<span class="math inline">\(B^{-1/2}AB^{-1/2}\)</span>，也即矩阵<span class="math inline">\(B^{-1}A\)</span>的最大特征值。而最小值为矩阵<span class="math inline">\(B^{-1}A\)</span>的最小特征值。</p>
<h2 id="lda原理">LDA原理</h2>
<h3 id="二分类原理">二分类原理</h3>
<p>  假设我们的数据集<span class="math inline">\(D = \{(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{m},y_{m})\}\)</span>。其中任意样本<span class="math inline">\(x_{i}\)</span>为n维向量，<span class="math inline">\(y_{i}\in\{0，1\}\)</span>。我们定义<span class="math inline">\(N_{j}(j=0,1)\)</span>为第j类样本的个数，<span class="math inline">\(X_{j}(j=0,1)\)</span>为第j类样本的集合，而<span class="math inline">\(\mu_{j}(j=0,1)\)</span>为第j类样本的均值向量，定义<span class="math inline">\(\sum_{j}(j=0,1)\)</span>为第j类样本的协方差矩阵（严格说是缺少分母部分的协方差矩阵）。<br> <span class="math inline">\(\mu_{j}\)</span>的表达式为：</p>
<p><span class="math display">\[\mu_{j} = \frac{1}{N_{j}}\sum_{x \in X_{j}}x(j=0,1)\]</span></p>
<p>  <span class="math inline">\(\sum_{j}\)</span>的表达式为：</p>
<p><span class="math display">\[\sum_{j} = \sum_{x \in X_{j}}(x-\mu_{j})(x-\mu_{j})^{T}(j=0,1)\]</span>   由于是两类数据，因此我们只需要将数据投影到一条直线上即可。假设我们的投影直线是向量w，则对任意一个样本<span class="math inline">\(x_{i}\)</span>，它在直线w的投影为<span class="math inline">\(w^{T}x_{i}\)</span>，对于我们的两个类别的中心点<span class="math inline">\(\mu_{0}\)</span>，<span class="math inline">\(\mu_{1}\)</span>，在直线w的投影为<span class="math inline">\(w^{T}\mu_{0}\)</span>和<span class="math inline">\(w^{T}\mu_{1}\)</span>。由于LDA需要让不同类别的数据的类别中心之间的距离尽可能的大，也就是我们要最大化<span class="math inline">\(||w^{T}\mu_{0}-w^{T}\mu_{1}||^{2}_{2}\)</span>，同时我们希望同一种类别数据的投影点尽可能的接近，也就是要同类样本投影点的协方差<span class="math inline">\(w^{T}\sum_{0}w\)</span>和<span class="math inline">\(w^{T}\sum_{1}w\)</span>尽可能的小，即最小化<span class="math inline">\(w^{T}\sum_{0}w + w^{T}\sum_{1}w\)</span>。<br></p>
<p>  综上所述，我们的优化目标为： <span class="math display">\[\mathop{\arg\max}_wJ(W)=\frac{||w^{T}\mu_{0}-w^{T}\mu_{1}||^{2}_{2}}{w^{T}\sum_{0}w + w^{T}\sum_{1}w} = \frac{w^{T}(\mu_{0}-\mu_{1})(\mu_{0}-\mu_{1})^{T}w}{w^{T}(\sum_{0}+\sum_{1})w}\]</span><br></p>
<p>  我们一般定义类内散度矩阵<span class="math inline">\(S_{w}\)</span>为：</p>
<p><span class="math display">\[S_{w} = \sum_{0}+\sum_{1} = \sum_{x \in X_{0}}(x - \mu_{0})(x - \mu_{0})^{T} + \sum_{x \in X_{1}}(x - \mu_{1})(x - \mu_{1})^{T}\]</span></p>
<p>  同时定义类间散度矩阵<span class="math inline">\(S_{b}\)</span>为： <span class="math display">\[S_{b}= (\mu_{0}-\mu_{1})(\mu_{0}-\mu_{1})^{T}\]</span></p>
<p>  这样，我们的优化目标重写为：</p>
<p><span class="math display">\[\mathop{\arg\max}_wJ(W)= \frac{w^{T}S_{b}w}{w^{T}S_{w}w}\]</span><br></p>
<p>  可以看出，上式即为广义瑞利商的形式，我们利用广义瑞利商的性质可知，<span class="math inline">\(J(w)\)</span>的最大值为矩阵<span class="math inline">\(S_{w}^{-1}S_{b}\)</span>的最大特征值，而对应的w为<span class="math inline">\(S_{w}^{-1}S_{b}\)</span>的最大特征值对应的特征向量。</p>
<p>  对于二类的时候，<span class="math inline">\(S_{b}w\)</span>的方向恒为<span class="math inline">\(\mu_{0}-\mu_{1}\)</span>，不妨令<span class="math inline">\(S_{b}w = \lambda(\mu_{0}-\mu_{1})\)</span>，将其带入：<span class="math inline">\((S_{w}^{-1}S_{b})w = \lambda w\)</span>，可以得到<span class="math inline">\(w = S_{w}^{-1}(\mu_{0}-\mu_{1})\)</span>，也就是说我们只要求出原始二类样本的均值和方差就可以确定最佳的投影方向w了。</p>
<h3 id="多分类原理">多分类原理</h3>
<p>  有了二类LDA的基础，我们再来看看多类别LDA的原理。<br><br>   假设我们的数据集 $ D = {(x_{1},y_{1}),,(x_{2},y_{2}),,...,,(x_{m},y_{m})}$ 其中任意样本<span class="math inline">\(x_{i}\)</span>为n维向量，<span class="math inline">\(y_{i}\in\{C_{1},\,C_{2},\,...,\,C_{k}\}\)</span>。我们定义<span class="math inline">\(N_{j}(j=1,2,...,k)\)</span>为第j类样本的个数，<span class="math inline">\(X_{j}(j=1,2,...,k)\)</span>为第j类样本的集合，而<span class="math inline">\(\mu_{j}(j=1,2,...,k)\)</span>为第j类样本的均值向量，定义为<span class="math inline">\(\sigma_{j}(j=1,2,...,k)\)</span>第j类样本的协方差矩阵。在二类LDA里面定义的公式可以很容易的类推到多类LDA。<br><br>   由于我们是多类向低维投影，则此时投影到的低维空间就不是一条直线，而是一个超平面。假设我们投影到的低维空间的维度为d，对应的基向量为<span class="math inline">\((w_{1},\,w_{2},\,...,\,w_{d})\)</span>，基向量的矩阵为<span class="math inline">\(W\)</span>，他是一个<span class="math inline">\(n\times d\)</span>的矩阵。<br>   此时我们的优化目标应该可以变成为： <span class="math display">\[\frac{W^T S_{b}W}{W^T S_{w}W}\]</span><br>   其中<span class="math inline">\(S_{b}=\sum_{j=1}^k N_{j}(\mu_j-\mu)(\mu_j-\mu)^T\)</span>,<span class="math inline">\(\mu\)</span>为所有样本的均值向量。而<span class="math inline">\(S_w=\sum_{j=1}^kS_{wj}=\sum_{j=1}^k\sum_{x\in X_j}(x-\mu_j)(x-\mu_j)^T\)</span>。<br><br>   但是有一个问题，就是<span class="math inline">\(W^T S_{b}W\)</span>和<span class="math inline">\(W^T S_{w}W\)</span>都是矩阵，不是标量，无法作为一个标量函数来优化！也就是说，我们无法直接用二类LDA的优化方法，怎么办呢？一般来说，我们可以用其他的一些替代优化目标来实现。<br><br>   常见的一个LDA多类优化目标函数定义为: <span class="math display">\[\mathop{\arg\max}_WJ(W)=\frac{\prod_{diag}W^TS_bW}{\prod_{diag}W^TS_wW}\]</span><br><br>   其中<span class="math inline">\(\prod_{diag}A\)</span>为<span class="math inline">\(A\)</span>的主对角线元素的乘积，<span class="math inline">\(W\)</span>为<span class="math inline">\(n\times d\)</span>的矩阵。<br><br>   <span class="math inline">\(J(W)\)</span>的优化过程可以转化为： <span class="math display">\[J(W)=\frac{\prod_{i=1}^dw_i^TS_bw_i}{\prod_{i=1}^dw_i^TS_ww_i}=\prod_{i=1}^d\frac{w_i^TS_bw_i}{w_i^TS_ww_i}\]</span> <br><br>   仔细观察上式最右边，这正是广义瑞利商。最大值是矩阵<span class="math inline">\(S_w^{-1}S_b\)</span>的最大特征值，最大的d个值的乘积就是矩阵<span class="math inline">\(S_w^{-1}S_b\)</span>的最大的d个特征值的乘积，此时对应的矩阵<span class="math inline">\(W\)</span>为这最大的d个特征值对应的特征向量张成的矩阵。<br><br>   由于<span class="math inline">\(W\)</span>是一个利用了样本的类别得到的投影矩阵，因此它降维到的维度d最大值为k-1。为什么最大维度不是类别数k呢？因为<span class="math inline">\(S_b\)</span>中每个<span class="math inline">\(\mu_j-\mu\)</span>的秩为1，因此协方差矩阵相加后最大的秩为k（矩阵的秩小于等于各个相加矩阵的秩的和），但是由于如果我们知道前k-1个<span class="math inline">\(\mu_j\)</span>后，最后一个<span class="math inline">\(\mu_k\)</span>可以由前k-1个<span class="math inline">\(\mu_j\)</span>线性表示，因此<span class="math inline">\(S_b\)</span>的秩最大为k-1，即特征向量最多由k-1个。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/02/05/HelloWord/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wuqiong'Zone">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/02/05/HelloWord/" itemprop="url">HelloWord</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-02-05T13:54:11+08:00">
                2018-02-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Joan</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Joan</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




<!-- 根据页面mathjax变量决定是否加载MathJax数学公式js -->
<% if (page.mathjax){ %>
<%- partial('mathjax') %>
<% } %>

        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
